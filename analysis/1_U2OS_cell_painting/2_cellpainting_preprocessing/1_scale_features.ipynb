{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comp Screening - cell painting Scale Features\n",
    "#### _BEM 09-20-2021_\n",
    "\n",
    "### Metadata\n",
    "* U2OS cells screened with single / full plate(s) of KI FDA library\n",
    "* Assayed with 5-color cell painting assay\n",
    "* 9x fields captured per well @ 20X magnification\n",
    "* All plates from all runs\n",
    "\n",
    "### Preprocessing\n",
    "Cell profiler used to image correct, capture QC metrics, segment, and feature extract (AWS), cytominer used to aggregate\n",
    "\n",
    "### What this does\n",
    "* 1) On a per batch basis\n",
    "    * 1) remove EMPTY wells and DMSO, TREAT wells < 50 cells\n",
    "    * 2) drop uninformative features (same value, 0s)\n",
    "    * 3) integrate all batches into single consistent full dataset\n",
    "    \n",
    "    \n",
    "* 2) On a per plate basis\n",
    "    * 1) scale each feature with sklearn RobustScaler\n",
    "    * 2) -OR- Perform transformation of features with sklearn PowerTransformer (Yeo-Johnson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "scale_path = '../1_SCALED_median_aggregated/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion and Scaling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for cleaning up pertubration names\n",
    "def remove_ion(lst):\n",
    "    lst = [x[:x.find('·')] if x.find('·') != -1 else x for x in lst]\n",
    "    return lst\n",
    "\n",
    "## function for gathering metadata and feature names and returning as set\n",
    "def extract_feat_meta(table):\n",
    "    \n",
    "    features = [col for col in \\\n",
    "                table.columns if 'Metadata' not in col and \\\n",
    "                'Number_of_Cells' not in col and \\\n",
    "                'Metadata_run' not in col]\n",
    "\n",
    "    metadata = [col for col in \\\n",
    "                table.columns if 'Metadata' in col and \\\n",
    "                'Metadata_run' not in col] \n",
    "    \n",
    "    return set(features), set(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for iterating scaling / transforming by plate\n",
    "def st_by_plate(table, batch, date, out_dir, oper, DMSO = False):\n",
    "    \n",
    "    ### INPUTS:\n",
    "    ## table - input data table in pandas\n",
    "    ## batch - str, batch name to append to output file\n",
    "    ## date - str, date to append to output file\n",
    "    ## oper - str, 'scale' applies robust scaling or 'transform' applies power transformation\n",
    "    ## DMSO - bool, if True, train on DMSO wells, transfrom all, if False train+trans all \n",
    "        \n",
    "    if oper == 'scale':\n",
    "        print('Starting feature scaling by plate')        \n",
    "    elif oper == 'transform':\n",
    "        print('Starting feature transformation by plate')\n",
    "    else:\n",
    "        print('ERROR - speficy valid oper argument')\n",
    "        return\n",
    "    \n",
    "    # load all plates, features\n",
    "    plates = list(set(table.Metadata_Plate.values))\n",
    "    features = [col for col in \\\n",
    "                    table.columns if 'Metadata' not in col and \\\n",
    "                    'Number_of_Cells' not in col]\n",
    "        \n",
    "    # run over plates\n",
    "    for p in plates:\n",
    "        if oper == 'scale':\n",
    "            table.loc[(table.Metadata_Plate == p), features] = robscale(p, table, features, DMSO)\n",
    "            \n",
    "        else:\n",
    "            table.loc[(table.Metadata_Plate == p), features] = ptrans(p, table, features, DMSO)\n",
    "        \n",
    "    print('Saving final tables')\n",
    "    if oper == 'scale' and DMSO == True:\n",
    "        save_stan = out_dir+date+'_robscaleDMSO_QC_median_'+batch+'_feature_table.gz'\n",
    "    elif oper == 'scale' and DMSO == False:\n",
    "        save_stan = out_dir+date+'_robscale_QC_median_'+batch+'_feature_table.gz'\n",
    "    elif oper == 'transform' and DMSO == False:\n",
    "        save_stan = out_dir+date+'_ptrans_QC_median_'+batch+'_feature_table.gz'\n",
    "    elif oper == 'transform' and DMSO == True:\n",
    "        save_stan = out_dir+date+'_ptransDMSO_QC_median_'+batch+'_feature_table.gz'\n",
    "    table.to_csv(save_stan, index=False, compression='gzip')\n",
    "    print('Finished with %s' %batch)\n",
    "    \n",
    "def robscale(p, table, features, DMSO):\n",
    "    \n",
    "    X = table.loc[(table.Metadata_Plate == p),(table.columns.isin(features))]\n",
    "    \n",
    "    if DMSO == True:\n",
    "        # extract dmso samples\n",
    "        X_DMSO = table.loc[(table.Metadata_Plate == p)&(table.Metadata_perturbation == 'DMSO'),\n",
    "                           (table.columns.isin(features))]\n",
    "        \n",
    "        # center all data by dmso\n",
    "        DMSO_scaler = RobustScaler(with_scaling = False, quantile_range = (0,100)).fit(X_DMSO)\n",
    "        X_center = DMSO_scaler.transform(X)\n",
    "        \n",
    "        # robust scale all data\n",
    "        scaler = RobustScaler(with_centering = False, quantile_range = (1,99),\n",
    "                              unit_variance = True).fit(X_center)\n",
    "        X_scale = scaler.transform(X_center)\n",
    "                \n",
    "    else:\n",
    "        # center and robust scale by all data\n",
    "        scaler = RobustScaler(quantile_range = (1,99), unit_variance = True).fit(X)\n",
    "        X_scale = scaler.transform(X)\n",
    "        \n",
    "    return X_scale\n",
    "    \n",
    "def ptrans(p, table, features, DMSO):\n",
    "    \n",
    "    X = table.loc[(table.Metadata_Plate == p),(table.columns.isin(features))]\n",
    "    \n",
    "    # power transform without scaling\n",
    "    transformer = PowerTransformer(standardize=False).fit(X)\n",
    "    trans_X = transformer.transform(X)\n",
    "    \n",
    "    if DMSO == True:\n",
    "        # extract dmso samples\n",
    "        X_DMSO = table.loc[(table.Metadata_Plate == p)&(table.Metadata_perturbation == 'DMSO'),\n",
    "                   (table.columns.isin(features))]\n",
    "        \n",
    "        # center all data by transformed dmso\n",
    "        trans_X_DMSO = transformer.transform(X_DMSO)\n",
    "        DMSO_scaler = RobustScaler(with_scaling = False, \n",
    "                                   quantile_range = (0,100)).fit(trans_X_DMSO)\n",
    "        trans_X_center = DMSO_scaler.transform(trans_X)\n",
    "        \n",
    "        # robust scale all data\n",
    "        scaler = RobustScaler(with_centering = False, quantile_range = (1,99),\n",
    "                              unit_variance = True).fit(trans_X_center)\n",
    "        trans_X_scale = scaler.transform(trans_X_center)\n",
    "    \n",
    "    else:\n",
    "        # center and robust scale by all transformed data\n",
    "        scaler = RobustScaler(unit_variance = True).fit(trans_X)\n",
    "        trans_X_scale = scaler.transform(trans_X)\n",
    "    \n",
    "    return trans_X_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load batch-wise median aggregated files, clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 2 ground truth batches\n",
    "\n",
    "# batch 1\n",
    "GT_run1_batch1 = pd.read_csv('10022020_QCfiltered_median_aggragated_batch_1_feature_table.gz',\n",
    "                             low_memory=False)\n",
    "\n",
    "# drop EMPTY wells, wells < 50 cells, wells with NaN features\n",
    "GT_run1_batch1 = GT_run1_batch1.loc[(GT_run1_batch1.Metadata_compound_name!='EMPTY') & (GT_run1_batch1.Number_of_Cells>50)].dropna()\n",
    "\n",
    "# Fix missing columns & names\n",
    "GT_run1_batch1['Metadata_compression'] = 1\n",
    "GT_run1_batch1['Metadata_replicates'] = 2\n",
    "GT_run1_batch1['Metadata_run'] = 'GT_run1_batch1'\n",
    "GT_run1_batch1.rename(columns={'Metadata_compound_name':'Metadata_perturbation'}, inplace=True)\n",
    "\n",
    "# get feats & metadata\n",
    "feat_gtr1b1, meta_gtr1b1 = extract_feat_meta(GT_run1_batch1)\n",
    "\n",
    "\n",
    "# batch 2\n",
    "GT_run1_batch2 = pd.read_csv('10022020_QCfiltered_median_aggragated_batch_2_feature_table.gz',\n",
    "                             low_memory=False)\n",
    "\n",
    "# drop EMPTY wells, wells < 50 cells, wells with NaN features\n",
    "GT_run1_batch2 = GT_run1_batch2.loc[(GT_run1_batch2.Metadata_compound_name!='EMPTY') & (GT_run1_batch2.Number_of_Cells>50)].dropna()\n",
    "\n",
    "# Fix missing columns & names\n",
    "GT_run1_batch2['Metadata_compression'] = 1\n",
    "GT_run1_batch2['Metadata_replicates'] = 2\n",
    "GT_run1_batch2['Metadata_run'] = 'GT_run1_batch2'\n",
    "GT_run1_batch2.rename(columns={'Metadata_compound_name':'Metadata_perturbation'}, inplace=True)\n",
    "\n",
    "# get feats & metadata\n",
    "feat_gtr1b2, meta_gtr1b2 = extract_feat_meta(GT_run1_batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 2 compressed screen batches\n",
    "\n",
    "# run 1\n",
    "CS_run1 = pd.read_csv('04122021_QCfiltered_median_aggragated_compressed_screen_run1_feature_table.gz',\n",
    "                      low_memory=False)\n",
    "\n",
    "# drop wells < 50 cells, wells with NaN features\n",
    "CS_run1 = CS_run1.loc[(CS_run1.Number_of_Cells>50)].dropna()\n",
    "\n",
    "# rename plates to match picklist names\n",
    "plate_ref = pd.read_csv('compressed_screen_run1.csv')\n",
    "plate_ref = plate_ref.set_index('Assay_Plate_Barcode').T.to_dict('list')\n",
    "\n",
    "for key in list(plate_ref.keys()):\n",
    "    CS_run1['Metadata_Plate'] = CS_run1['Metadata_Plate'].replace(key,str(plate_ref[key][0]))\n",
    "\n",
    "# drop FAILED ECHO plates\n",
    "# plate1 == 2x3r random\n",
    "drop_plate = ['plate1']\n",
    "CS_run1 = CS_run1.loc[~(CS_run1.Metadata_Plate.isin(drop_plate))]\n",
    "    \n",
    "# Fix missing columns\n",
    "CS_run1['Metadata_KI_ID'] = 'NA'\n",
    "CS_run1['Metadata_run'] = 'CS_run1'\n",
    "\n",
    "# get feats & metadata\n",
    "feat_csr1, meta_csr1 = extract_feat_meta(CS_run1)\n",
    "\n",
    "# run 2\n",
    "CS_run2 = pd.read_csv('04122021_QCfiltered_median_aggragated_compressed_screen_run2_feature_table.gz',\n",
    "                      low_memory=False)\n",
    "\n",
    "# drop wells < 50 cells, wells with NaN features\n",
    "CS_run2 = CS_run2.loc[(CS_run2.Number_of_Cells>50)].dropna()\n",
    "\n",
    "# rename plates to match picklist names\n",
    "plate_ref = pd.read_csv('compressed_screen_run2.csv')\n",
    "plate_ref = plate_ref.set_index('Assay_Plate_Barcode').T.to_dict('list')\n",
    "\n",
    "for key in list(plate_ref.keys()):\n",
    "    CS_run2['Metadata_Plate'] = CS_run2['Metadata_Plate'].replace(key,str(plate_ref[key][0]))\n",
    "\n",
    "# drop FAILED ECHO plates\n",
    "# plate1 == 2x3r random\n",
    "drop_plate = ['plate1']\n",
    "CS_run2 = CS_run2.loc[~(CS_run2.Metadata_Plate.isin(drop_plate))]\n",
    "    \n",
    "# Fix missing columns\n",
    "CS_run2['Metadata_KI_ID'] = 'NA'\n",
    "CS_run2['Metadata_run'] = 'CS_run2'\n",
    "\n",
    "# get feats & metadata\n",
    "feat_csr2, meta_csr2 = extract_feat_meta(CS_run2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final comp screen & ground truth batches\n",
    "run3 = pd.read_csv('06232021_QCfiltered_median_aggragated_compressed_screen_run3_feature_table.gz',\n",
    "                   low_memory=False)\n",
    "\n",
    "# fill missing metadata for ground truth & compressed plates\n",
    "run3.Metadata_compression.fillna(1, inplace=True)\n",
    "run3.Metadata_replicates.fillna(4, inplace=True)\n",
    "run3.Metadata_KI_ID.fillna(\"NA\", inplace=True)\n",
    "\n",
    "# drop extra index rows (if present)\n",
    "run3.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# drop wells < 50 cells, wells with NaN features\n",
    "run3 = run3.loc[(run3.Number_of_Cells>50)].dropna()\n",
    "\n",
    "# rename plates to match picklist names\n",
    "plate_ref = pd.read_csv('compressed_screen_run3_plate_rename.csv')\n",
    "plate_ref = plate_ref.set_index('Assay_Plate_Barcode').T.to_dict('list')\n",
    "\n",
    "for key in list(plate_ref.keys()):\n",
    "    run3['Metadata_Plate'] = run3['Metadata_Plate'].replace(key,str(plate_ref[key][0]))\n",
    "\n",
    "# Fix perturbation name before separating\n",
    "run3.Metadata_perturbation = remove_ion(run3.Metadata_perturbation)\n",
    "run3.loc[run3.Metadata_KI_ID=='KI-ENZO-FDA-299', 'Metadata_perturbation'] = 'Methyldopa Sesquihydrate (L-Α-Methyl-Dopa Sesquihyrate)'\n",
    "\n",
    "# Split into compressed screen and ground truth\n",
    "CS_run3 = run3[run3['Metadata_KI_ID']=='NA'].reset_index(drop=True)\n",
    "GT_run2 = run3[run3['Metadata_KI_ID']!='NA'].reset_index(drop=True)\n",
    "del run3\n",
    "\n",
    "# Fix missing columns\n",
    "GT_run2['Metadata_run'] = 'GT_run2'\n",
    "CS_run3['Metadata_run'] = 'CS_run3'\n",
    "\n",
    "# drop EMPTY wells\n",
    "GT_run2 = GT_run2.loc[(GT_run2.Metadata_perturbation!='EMPTY')].reset_index(drop=True)\n",
    "\n",
    "# get feats & metadata\n",
    "feat_gtr2, meta_gtr2 = extract_feat_meta(GT_run2)\n",
    "feat_csr3, meta_csr3 = extract_feat_meta(CS_run3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged dataset has 20686 samples and 3402 features\n"
     ]
    }
   ],
   "source": [
    "# define mutual features and metadata across batches\n",
    "features = list(set.intersection(feat_gtr1b1, feat_gtr1b2, feat_csr1, feat_csr2, feat_gtr2, feat_csr3))\n",
    "metadata = list(set.intersection(meta_gtr1b1, meta_gtr1b2, meta_csr1, meta_csr2, meta_gtr2, meta_csr3))\n",
    "\n",
    "# merge into one big data table\n",
    "frame = [GT_run1_batch1, GT_run1_batch2, GT_run2, CS_run1, CS_run2, CS_run3]\n",
    "data = pd.concat(frame)[['Metadata_run'] + metadata + ['Number_of_Cells'] + features]\n",
    "data.Metadata_Plate = data.Metadata_run + '_' + data.Metadata_Plate\n",
    "\n",
    "# drop features with a single value\n",
    "nunique = data.nunique()\n",
    "cols_to_drop = nunique[data.nunique() == 1].index\n",
    "data = data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print('merged dataset has '+str(data.shape[0])+' samples and '+str(data.shape[1])+ ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-merged datasets\n",
    "del GT_run1_batch1, GT_run1_batch2, GT_run2, CS_run1, CS_run2, CS_run3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged dataset\n",
    "data.to_csv('09202021_QC_median_ALL_feature_table.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('09202021_QC_median_ALL_feature_table.gz',\n",
    "                   low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature scaling by plate\n",
      "Saving final tables\n",
      "Finished with all\n"
     ]
    }
   ],
   "source": [
    "st_by_plate(data, 'all', '0920201', scale_path, oper = 'scale', DMSO = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
